# 1.  Historia badań nad sztuczną inteligencją

Kiedy system jest inteligentny?

Próbował odpowiedzieć w 1950 roku Alan M. Turing, zaproponował on gre imitacyjną.
**Gra imitacyjna** to test, który polega na tym, że człowiek tester prowadzi równoczeście dialog z komputerem i człowiek nie wiedziąc który, to który, jeżeli tester nie rozróżni, to zdaniem Turinga SI jest nierozróżnialne od ludzkiej.

1956 - powszechnie przyjmuje się narodziny SI, konferencja w Dartmouth

1955 - wg. autora początek SI, wtedy powstał **Logic Theorist**, który dowiódł blisko 40 twierdzeń z Principia Mathematica ( A. Newell, H. A. Simon)

1957 - **General Problem Solver** - rozwiązywał On rozmaite problemy, np. wieże hanoi, całkowanie symboliczne (H. A. Simon, B. Russel)

A. Newell i H. A. Simon zdefiniowali nowe podejście nazywane **symulacją kognitywną**, w tej architekturze należy symulować ludzkie procesy umysłowo-poznawcze

1958-1960 - stworzenie **Lisp'a** przez J. McCarthiego ( inspirowany rachunkiem lambda)

1973 - stworzenie **Prologu** A. Colmeraurera, P. Roussela ( inspirowany logiką pierwszego rzędu) 

połowa lat sześćdziesiątych XX wieku - powstanie **podejście opartego na wiedzy** - kierunek związany z projektem DENDRAl (E. Fegienbaum, J Lederberg).
System ustalał strukturę molekularną nieznanych związków organiczntch na podstawie analizy widm spektroskopowych.
Paradygmat ten charakteryzuje się tym, że jest wyspecjalizowany jedynie  w pewnej dziedzinie, jest mu dostarczona wiedza ludzi-eksperów, wiedza powinna być sformalizowana w postaci danych.
System powinien wnioskować równolegle z ludźmi-ekspertami.

1976 - zdefiniowanie przez ( A. Newella i H. A. Simona ) **Fizycznego systemu symbolicznego** - 
składa się ze zbioru elementów zwanych też symbolami, z których konstruuje on struktury symboliczne, zwane wyrażeniami - oraz zbioru procesów operującyh na tych wyrażeniach. Oryginalnie:
> Fizyczny system symboliczny ma konieczne i wystarczające środki do generowania inteligentnego działania.

1980 - Podział poglądów na temat sztucznej inteligencji na:
- **słabe podejście do sztucznej inteligencji** - traktują komputer jako wygodne narzędzie do formułowania i testowania hipotez dotyczących mózgu oraz do symulacji czynności umysłu.
- **silne podejście do sztucznej inteligencji** - uważają, że odpowiednio zaprogramowany komputer jest równoważny mózgowi wraz z jego czynnościami umysłowymi

Aby odrzucić hipotezę o dotyczącą fizycznego systemu symbolicznego J.R Searle zaproponował eksperyment zwany **Chińskim pokojem** (s. 7)

1957 - pierwsza wersja **teorii gramatyk generatywnych** N. Chomskiego.
Zgodnie z tą teorią w umyśle istnieje meta-wiedza językowa nazywana gramatyką uniwersalną, która w trakcie dorastania jest "parametryzowana" zdaniami konkretnych języków. Za pomocą tej "sparametryzowanej" gramatyki można utworzyć nieskończoną ilość zdań tego języka.

koniec lat 60 XX wieku - **Teoria zależności pojęciowej** R. Schanka. 
Nie syntaktyka powinna być punktem wyjścia do analizy semantycznej, a pojęcia. W tym celu stworzył *diagramy zależności pojęciowej*, w których rózne struktury językowe mające to samo znaczenie mają taką samą reprezentacje niezależnie od języka.

1987 - **lingwistyka kognitywna** G. Lakoffa - zadał On pytanie (a propos fizycznego systemu symbolicznego) : 
> Czy możliwe jest odwzorowanie otaczającej nas rzeczywistości w zbiór takich wyrażeń?
W tym podejściu nie przyporządkowuje się niczego do danej kategorii, ale możliwe jest aby przynależność była "rozmyta". (s. 10)

**modele konekcjonistyczne** - zjawiska umysłowe modeluje się jako procesy zachodzące w sieciach złożonych z prostych elementów składowych np. sieć neuronowa.

**modele inspirowane biologią** np.
- **obliczenia ewolucyjne**
Modele te służą głównie do poszukiwania optymalnego rozwiązania, traktują przestrzeń możliwych rozwiązań jako "osobników".
Istnieją cztery podstawowe grupy metod:
- **algorytmy genetyczne**
- **strategie ewolucyjne**
- **programowanie ewolucyjne**
- **programowanie genetyczne**

**rozpoznawanie wzorców(obrazów)** - służy do klasyfikacji nieznanych obiektów reprezentowanych jako zespoły cech.

**wnioskowanie** oparte na **sieciach bayesowskich** pozwala rozwiązać problem z informacjami niepewnymi

**teoria zbiorów rozmytych** oraz **teoria zbiorów przybliżonych** pozwala się uporać z problemem niejednoznaczności opisów naszego świata.

**architektury kognitywne** oraz **systemy wieloagentowe** służą integracji wielu metod sztucznej inteligencji
